---
title: "Anomaly/outlier detection"
author: "Adithya Harish"
image: "anomaly.png"
---

<table align="left">
  <td>
    <a href="https://colab.research.google.com/drive/1DmLUWuX_N5lznzTXl9mMhLhYBKyquFi7?usp=sharing" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
  </td>
</table>

# Anomaly/outlier detection

Anomaly or outlier detection in machine learning refers to the identification of rare items, events, or observations which raise suspicions by differing significantly from the majority of the data. Typically, these anomalies represent issues such as errors, frauds, faults, or significant deviations from the norm. In the context of machine learning, anomaly detection is applied across various domains like fraud detection in banking, intrusion detection in network security, fault detection in manufacturing, and medical diagnosis.

The process involves training a model to identify the patterns of normal data and subsequently detect data points that deviate from this normal pattern.


## Step 1: Importing the Dataset

The `Glass Identification` Dataset is a dataset containing information about different types of glass. We can import it from the ODDS platform using the pandas library.

```{python}
import scipy.io
import pandas as pd

# Load the .mat file
mat_file_path = 'glass.mat'  # Replace with the actual path
mat_data = scipy.io.loadmat(mat_file_path)

# Extract 'X' and 'y' from the .mat file
X = mat_data['X']
y = mat_data['y']

# Displaying the first few rows of 'X' and 'y' for step 1
pd.DataFrame(X, columns=[f"Feature_{i+1}" for i in range(X.shape[1])]).head(), pd.DataFrame(y, columns=['Target']).head()

```

## Step 2: Data Pre-processing

In this step, we handle missing values and prepare the data for the model. This may include filling missing values, encoding categorical variables, and scaling features.
```{python}
from sklearn.preprocessing import MinMaxScaler

# Convert 'X' and 'y' into pandas DataFrames
df_X = pd.DataFrame(X)
df_y = pd.DataFrame(y)

# Check for missing values
missing_values = df_X.isnull().sum().sum() + df_y.isnull().sum().sum()

# Normalize the feature data
scaler = MinMaxScaler()
df_X_scaled = scaler.fit_transform(df_X)
```

## Step 3: Splitting the Test and Train Sets

We divide the dataset into training and testing sets.

```{python}
from sklearn.model_selection import train_test_split

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(df_X_scaled, df_y, test_size=0.2, random_state=42)

```

## Step 4: Fitting the Model to the Training Set

We use the Isolation Forest model for anomaly detection.
The Isolation Forest method is a popular algorithm used for anomaly detection. The Isolation Forest isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.

```{python}
from sklearn.ensemble import IsolationForest

# Fit the Isolation Forest model to the training set
iso_forest = IsolationForest(contamination=0.1, random_state=42)
iso_forest.fit(X_train)

```

## Step 5: Predicting Test Results

Now that our model is trained, we can use it to predict anomalies in the test set.

```{python}
# Predict anomalies in the test set
predictions = iso_forest.predict(X_test)

# Add predictions to the test set for visualization
X_test_with_predictions = pd.DataFrame(X_test).copy()
X_test_with_predictions['Anomaly'] = predictions

```

## Step 6: Visualizing the Test Results

To visualize the results of our anomaly detection model, we can create a scatter plot to show the anomalies in the test data.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Selecting two features for the scatter plot
feature_1_index = 0  # First feature
feature_2_index = 1  # Second feature

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_test_with_predictions.iloc[:, feature_1_index], 
                y=X_test_with_predictions.iloc[:, feature_2_index], 
                hue=X_test_with_predictions['Anomaly'], 
                palette=['red', 'green'], alpha=0.7)
plt.title('Scatter Plot of Test Data: Feature 1 vs Feature 2')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(title='Anomaly', labels=['Outlier', 'Inlier'])
plt.show()

```

## Testing the accuracy

```{python}
from sklearn.metrics import accuracy_score

# Converting the anomaly predictions from [-1, 1] to [0, 1], where -1 (anomaly) becomes 1 and 1 (normal) becomes 0
# This is to align with the target variable's encoding where we assume 1 represents anomaly and 0 represents normal
predicted_labels = (predictions == -1).astype(int)

# Calculating the accuracy
accuracy = accuracy_score(y_test, predicted_labels)
accuracy

```